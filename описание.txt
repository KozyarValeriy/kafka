
Задание
1. сделать продюсер, который сгенерит 1млн записей и забросит в какой то топик кафки
2. сделать в стринге, json, avro
3. сделать тестовый получатель, чтобы просто был вывод значений (который делает обратный процесс - парсинг)
4. далее сделать получателя на спарк стриминг (окно по 5 секунд) и в окне реализовать то действие, которое было в запросе (агрегация)
(сумма, среднее и тд)
5. запись информации в базу данных. соеденить каждые секундные интервалы в одно целое

------------------------------------
----- Типы файлов при передаче -----
------------------------------------

1. TYPE = 'CSV'
    Значения разделены запятыми:
    name,shop,traffic,success_sell

2. TYPE = 'JSON'
    Значения в формате json:
    {name: "name", shop: "shop", traffic: "traffic", success_sell: "success_sell"}

3. TYPE = 'AVRO'
    Схема для данных:
    {"type": "record",
     "name": "Worker",
     "fields": [
         {"name": "name", "type": "string"},
         {"name": "shop", "type": "int"},
         {"name": "traffic", "type": "int"},
         {"name": "success_sell", "type": "int"}
        ]
    }


-- для подключения к своей базе
/home/usertest/sqlcl/bin/sql kozyar/usertest@192.168.88.200:1521:orcl


-- запуск файлов на сервере:
-- переходим в папку
cd kozyar/kafka

-- добавляем права на исполнение
chmod +x producer.py
chmod +x consumer_simple.py
chmod +x consumer.py


-- запуск производителя
python3.6 producer.py


-- запуск различных получателей
spark-submit --master local[1] consumer_simple.py
spark-submit --master local[1] consumer.py
spark-submit --master local[1] --packages org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.4.3 consumer_spark.py

